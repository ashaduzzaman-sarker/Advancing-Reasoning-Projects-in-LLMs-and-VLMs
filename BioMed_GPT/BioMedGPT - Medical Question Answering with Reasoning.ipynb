{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8c8bb773b23849a9b0b034c81d4990d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e98ee76a41504c72b59bf6d28df665c2",
              "IPY_MODEL_9b7811bfc21a4fd5ae957e3808056568",
              "IPY_MODEL_6c1dc470ffc94391b010739e8dfc5154"
            ],
            "layout": "IPY_MODEL_782be50f3ac546a38eff9d82a43b7c4b"
          }
        },
        "e98ee76a41504c72b59bf6d28df665c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5107bff48da40e28c6661894074ad39",
            "placeholder": "​",
            "style": "IPY_MODEL_7446a74db19b47328f15c5e4d46c24af",
            "value": ""
          }
        },
        "9b7811bfc21a4fd5ae957e3808056568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77d76133e0ff4572949bd344c4c658ca",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d6487dd85d342d89378f0328de360c2",
            "value": 1
          }
        },
        "6c1dc470ffc94391b010739e8dfc5154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6695c15ee63543c69ed1c0032c0a7536",
            "placeholder": "​",
            "style": "IPY_MODEL_93f3276f8a5843619af81790488aab3f",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:25&lt;00:00, 25.98s/it]\n"
          }
        },
        "782be50f3ac546a38eff9d82a43b7c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5107bff48da40e28c6661894074ad39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7446a74db19b47328f15c5e4d46c24af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77d76133e0ff4572949bd344c4c658ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d6487dd85d342d89378f0328de360c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6695c15ee63543c69ed1c0032c0a7536": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f3276f8a5843619af81790488aab3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64283ab760844eb1858e7cdc540ed8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98688bf0279a4c56bc493c2f6a90d8e5",
              "IPY_MODEL_a0c85c443a5e4223b16ca7d88229a825",
              "IPY_MODEL_c6d932823ff04acbbba46d728d5b51ab"
            ],
            "layout": "IPY_MODEL_326786266ba4440b80a60958cd2b42dd"
          }
        },
        "98688bf0279a4c56bc493c2f6a90d8e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1421a600e1c41d2a0ad4054c54932ac",
            "placeholder": "​",
            "style": "IPY_MODEL_262a0899b68f418aaa1c4f4370fca55f",
            "value": ""
          }
        },
        "a0c85c443a5e4223b16ca7d88229a825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba4b15c0cd574f5ebfeb05c884f70089",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_279b3eb7e59c41c6bd9fb4a9c3e17ef4",
            "value": 1
          }
        },
        "c6d932823ff04acbbba46d728d5b51ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58aefd51782c4fba889825b7c9f1e1ba",
            "placeholder": "​",
            "style": "IPY_MODEL_4638a4e1d4ad4b83966f8c193994a6f5",
            "value": "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:20&lt;00:00, 20.67s/it]\n"
          }
        },
        "326786266ba4440b80a60958cd2b42dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1421a600e1c41d2a0ad4054c54932ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "262a0899b68f418aaa1c4f4370fca55f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba4b15c0cd574f5ebfeb05c884f70089": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "279b3eb7e59c41c6bd9fb4a9c3e17ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58aefd51782c4fba889825b7c9f1e1ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4638a4e1d4ad4b83966f8c193994a6f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "4O4CjoewquAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth vllm\n",
        "!pip install --upgrade pillow"
      ],
      "metadata": {
        "id": "uU9puvBSGBXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "clo0vkg4q467"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDS6A3XVQEA3",
        "outputId": "19d9f2d7-2367-4fdc-b913-0cd962453a94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mashaduzzaman2505\u001b[0m (\u001b[33mashaduzzaman_sarker\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "T9aDCN55QIuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Datasets (all splits)"
      ],
      "metadata": {
        "id": "ByXI537PZxDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from datasets import load_dataset\n",
        "from datasets import concatenate_datasets"
      ],
      "metadata": {
        "id": "gGwrD1yvZoZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MedQA-USMLE (if available on Hugging Face)\n",
        "medqa_usmle = load_dataset(\"Neelectric/MedQA-USMLE\")\n",
        "print(\"MedQA-USMLE loaded successfully.\")\n",
        "medqa_usmle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tHoFF6Zq72q",
        "outputId": "e5dea6f7-b088-4283-a9fc-12c8b04dbf43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedQA-USMLE loaded successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
              "        num_rows: 10178\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
              "        num_rows: 1272\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx'],\n",
              "        num_rows: 1273\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MedMCQA\n",
        "medmcqa = load_dataset(\"openlifescienceai/medmcqa\")\n",
        "print(\"MedMCQA loaded successfully.\")\n",
        "medmcqa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6Sk3wOerpVE",
        "outputId": "14dd9dd9-d16f-4280-b708-86c11055d6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MedMCQA loaded successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
              "        num_rows: 182822\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
              "        num_rows: 6150\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n",
              "        num_rows: 4183\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKGOKYpvqlCk",
        "outputId": "fc7e4969-f2a8-4c70-e215-a36db3a6b88d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PubMedQA loaded successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
              "        num_rows: 200000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['QUESTION', 'CONTEXTS', 'LABELS', 'MESHES', 'YEAR', 'reasoning_required_pred', 'reasoning_free_pred', 'final_decision', 'LONG_ANSWER'],\n",
              "        num_rows: 11269\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Load PubMedQA for additional medical domain data\n",
        "pubmed_qa = load_dataset(\"bigbio/pubmed_qa\")\n",
        "print(\"PubMedQA loaded successfully.\")\n",
        "pubmed_qa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MMLU for general-domain verification questions\n",
        "mmlu = load_dataset(\"TIGER-Lab/MMLU-Pro\")\n",
        "print(\"MMLU loaded successfully.\")\n",
        "mmlu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMilhDD7sLWb",
        "outputId": "ded07e45-8b96-4c9c-f357-0af3bb416a9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MMLU loaded successfully.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    test: Dataset({\n",
              "        features: ['question_id', 'question', 'options', 'answer', 'answer_index', 'cot_content', 'category', 'src'],\n",
              "        num_rows: 12032\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['question_id', 'question', 'options', 'answer', 'answer_index', 'cot_content', 'category', 'src'],\n",
              "        num_rows: 70\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Unified verifiable dataset"
      ],
      "metadata": {
        "id": "sznYmn23_g25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all splits of a DatasetDict\n",
        "\n",
        "def merge_all_splits(dataset_dict):\n",
        "    \"\"\"\n",
        "    Merge all splits from a DatasetDict into a single Dataset.\n",
        "    \"\"\"\n",
        "    datasets_list = [dataset_dict[k] for k in dataset_dict.keys()]  # Convert to list\n",
        "    merged_dataset = concatenate_datasets(datasets_list)  # Use concatenate_datasets\n",
        "    return merged_dataset\n",
        "\n",
        "\n",
        "# Merge splits for each dataset\n",
        "merged_medqa = merge_all_splits(medqa_usmle)\n",
        "merged_medmcqa = merge_all_splits(medmcqa)\n",
        "merged_pubmed = merge_all_splits(pubmed_qa)\n",
        "merged_mmlu = merge_all_splits(mmlu)"
      ],
      "metadata": {
        "id": "_PQYmxvsZ5ZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_medqa[13]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AURlO-I0p4ln",
        "outputId": "3a13bf3d-4b12-45e0-d63f-c976948d0f91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'A 23-year-old man comes to the physician for evaluation of decreased hearing, dizziness, and ringing in his right ear for the past 6 months. Physical examination shows multiple soft, yellow plaques and papules on his arms, chest, and back. There is sensorineural hearing loss and weakness of facial muscles bilaterally. His gait is unsteady. An MRI of the brain shows a 3-cm mass near the right internal auditory meatus and a 2-cm mass at the left cerebellopontine angle. The abnormal cells in these masses are most likely derived from which of the following embryological structures?',\n",
              " 'answer': 'Neural crest',\n",
              " 'options': {'A': 'Neural tube',\n",
              "  'B': 'Surface ectoderm',\n",
              "  'C': 'Neural crest',\n",
              "  'D': 'Notochord',\n",
              "  'E': 'Mesoderm'},\n",
              " 'meta_info': 'step1',\n",
              " 'answer_idx': 'C'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reformatting functions for each dataset"
      ],
      "metadata": {
        "id": "MwqWFCyiZ-vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reformat_medqa(item):\n",
        "    \"\"\"\n",
        "    For MedQA-USMLE items with fields: 'question', 'answer', 'options', 'answer_idx'\n",
        "    \"\"\"\n",
        "    x = item[\"question\"]\n",
        "    y_star = None\n",
        "    if \"answer\" in item and item[\"answer\"]:\n",
        "        y_star = item[\"answer\"].strip()\n",
        "    elif \"answer_idx\" in item and \"options\" in item:\n",
        "        idx = item[\"answer_idx\"]\n",
        "        if idx is not None and idx < len(item[\"options\"]):\n",
        "            y_star = chr(ord('A') + idx)\n",
        "    return {\"x\": x, \"y_star\": y_star}\n",
        "\n",
        "def reformat_medmcqa(item):\n",
        "    \"\"\"\n",
        "    Reformat MedMCQA items into verifiable medical problems.\n",
        "    - `question`: The question text.\n",
        "    - `opa`, `opb`, `opc`, `opd`: Answer choices.\n",
        "    - `cop`: Index of the correct answer (integer).\n",
        "    \"\"\"\n",
        "    x = item[\"question\"]\n",
        "    options = [item[\"opa\"], item[\"opb\"], item[\"opc\"], item[\"opd\"]]\n",
        "\n",
        "    # Convert integer answer index to a letter (A, B, C, D)\n",
        "    if \"cop\" in item and isinstance(item[\"cop\"], int) and 0 <= item[\"cop\"] < len(options):\n",
        "        y_star = chr(ord('A') + item[\"cop\"])  # Convert 0->'A', 1->'B', etc.\n",
        "    else:\n",
        "        y_star = None\n",
        "\n",
        "    return {\"x\": x, \"y_star\": y_star, \"options\": options}\n",
        "\n",
        "\n",
        "def reformat_pubmed(item):\n",
        "    \"\"\"\n",
        "    For PubMedQA items with fields: 'QUESTION', 'final_decision'\n",
        "    \"\"\"\n",
        "    x = item[\"QUESTION\"]\n",
        "    y_star = item[\"final_decision\"].strip() if \"final_decision\" in item and item[\"final_decision\"] else None\n",
        "    return {\"x\": x, \"y_star\": y_star}\n",
        "\n",
        "def reformat_mmlu(item):\n",
        "    \"\"\"\n",
        "    For MMLU items with fields: 'question', 'answer', 'options', 'answer_index'\n",
        "    \"\"\"\n",
        "    x = item[\"question\"]\n",
        "    y_star = None\n",
        "    if \"answer\" in item and item[\"answer\"]:\n",
        "        y_star = item[\"answer\"].strip()\n",
        "    elif \"answer_index\" in item and \"options\" in item:\n",
        "        idx = item[\"answer_index\"]\n",
        "        if idx is not None and idx < len(item[\"options\"]):\n",
        "            y_star = chr(ord('A') + idx)\n",
        "    return {\"x\": x, \"y_star\": y_star}"
      ],
      "metadata": {
        "id": "sMyu5mBPaEKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtering Function"
      ],
      "metadata": {
        "id": "gj59jkreaF3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_challenging(item, min_word_count=30):\n",
        "    \"\"\"\n",
        "    Keep only items with a question length above a minimum threshold.\n",
        "    \"\"\"\n",
        "    question = item.get(\"x\", \"\")\n",
        "    return len(question.split()) >= min_word_count"
      ],
      "metadata": {
        "id": "TKhV-Kw_aIhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Unified Verifiable Dataset"
      ],
      "metadata": {
        "id": "cz4zB58GaKsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_verifiable_dataset():\n",
        "    verifiable = []\n",
        "\n",
        "    # Process MedQA-USMLE\n",
        "    for item in merged_medqa:\n",
        "        reformatted = reformat_medqa(item)\n",
        "        if reformatted[\"y_star\"]:\n",
        "            verifiable.append(reformatted)\n",
        "\n",
        "    # Process MedMCQA\n",
        "    for item in merged_medmcqa:\n",
        "        reformatted = reformat_medmcqa(item)\n",
        "        if reformatted[\"y_star\"]:\n",
        "            verifiable.append(reformatted)\n",
        "\n",
        "    # Process PubMedQA\n",
        "    for item in merged_pubmed:\n",
        "        reformatted = reformat_pubmed(item)\n",
        "        if reformatted[\"y_star\"]:\n",
        "            verifiable.append(reformatted)\n",
        "\n",
        "    # Process MMLU\n",
        "    for item in merged_mmlu:\n",
        "        reformatted = reformat_mmlu(item)\n",
        "        if reformatted[\"y_star\"]:\n",
        "            verifiable.append(reformatted)\n",
        "\n",
        "    # Optionally, filter out examples with very short questions\n",
        "    verifiable = [ex for ex in verifiable if filter_challenging(ex)]\n",
        "\n",
        "    return verifiable\n",
        "\n",
        "verifiable_dataset = create_verifiable_dataset()\n",
        "print(\"Unified verifiable dataset size:\", len(verifiable_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWgipg5_9tpZ",
        "outputId": "9f42034c-4a3b-4342-d5b4-b2e72e65131c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unified verifiable dataset size: 33929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "verifiable_dataset[12]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p91TaFoJ_LSB",
        "outputId": "4848578c-3ee3-481e-ea16-bc2201f8b6a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x': 'A 9-month-old female is brought to the emergency department after experiencing a seizure. She was born at home and was normal at birth according to her parents. Since then, they have noticed that she does not appear to be achieving developmental milestones as quickly as her siblings, and often appears lethargic. Physical exam reveals microcephaly, very light pigmentation (as compared to her family), and a \"musty\" body odor. The varied manifestations of this disease can most likely be attributed to which of the following genetic principles?',\n",
              " 'y_star': 'Pleiotropy'}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Develop a Medical Verifier"
      ],
      "metadata": {
        "id": "WZmhgIxm_kJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "class MedicalVerifier:\n",
        "    def __init__(self):\n",
        "        # Regex patterns to extract content within <think> and <answer> tags.\n",
        "        self.think_pattern = r\"<think>(.*?)</think>\"\n",
        "        self.answer_pattern = r\"<answer>(.*?)</answer>\"\n",
        "\n",
        "    def verify_format(self, model_output):\n",
        "        \"\"\"\n",
        "        Verify that the model output follows the required format:\n",
        "         - Exactly one <think>...</think> block.\n",
        "         - Exactly one <answer>...</answer> block.\n",
        "         - No extra non-whitespace text exists outside these tags.\n",
        "\n",
        "        Returns:\n",
        "            (bool, str): A tuple containing whether the format is correct and an error message (if any).\n",
        "        \"\"\"\n",
        "        # Find all occurrences of the required tags.\n",
        "        think_matches = re.findall(self.think_pattern, model_output, re.DOTALL)\n",
        "        answer_matches = re.findall(self.answer_pattern, model_output, re.DOTALL)\n",
        "\n",
        "        if len(think_matches) != 1 or len(answer_matches) != 1:\n",
        "            return False, \"Incorrect number of <think> or <answer> tags.\"\n",
        "\n",
        "        # Remove the tags from the output.\n",
        "        cleaned_output = re.sub(self.think_pattern, \"\", model_output, flags=re.DOTALL)\n",
        "        cleaned_output = re.sub(self.answer_pattern, \"\", cleaned_output, flags=re.DOTALL)\n",
        "        # Remove all whitespace characters.\n",
        "        cleaned_output = cleaned_output.strip()\n",
        "\n",
        "        # Check if any non-whitespace text remains.\n",
        "        if cleaned_output and not cleaned_output.isspace():\n",
        "            return False, \"Extra text found outside of the required tags.\"\n",
        "\n",
        "        return True, \"\"\n",
        "\n",
        "    def extract_answer(self, model_output):\n",
        "        \"\"\"\n",
        "        Extract the final answer from the <answer>...</answer> tag.\n",
        "\n",
        "        Returns:\n",
        "            str or None: The extracted answer with whitespace stripped, or None if not found.\n",
        "        \"\"\"\n",
        "        match = re.search(self.answer_pattern, model_output, re.DOTALL)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "        return None\n",
        "\n",
        "    def verify(self, model_output, ground_truth):\n",
        "        \"\"\"\n",
        "        Verify the model's output by:\n",
        "          1. Checking that the format is correct (exactly one <think> block and one <answer> block with no extra text).\n",
        "          2. Extracting and comparing the final answer against the ground-truth.\n",
        "\n",
        "        Args:\n",
        "            model_output (str): The complete output from the model.\n",
        "            ground_truth (str): The expected correct answer (e.g., \"C\" or \"Cerebral edema\").\n",
        "\n",
        "        Returns:\n",
        "            bool: True if both format and answer match the ground truth, otherwise False.\n",
        "        \"\"\"\n",
        "        # Step 1: Verify format.\n",
        "        format_ok, error_message = self.verify_format(model_output)\n",
        "        if not format_ok:\n",
        "            print(\"Format error:\", error_message)\n",
        "            return False\n",
        "\n",
        "        # Step 2: Extract and verify answer.\n",
        "        extracted_answer = self.extract_answer(model_output)\n",
        "        if extracted_answer is None:\n",
        "            print(\"No answer found in the output.\")\n",
        "            return False\n",
        "\n",
        "        if extracted_answer == ground_truth:\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"Answer mismatch: extracted '{extracted_answer}', expected '{ground_truth}'\")\n",
        "            return False\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    verifier = MedicalVerifier()\n",
        "\n",
        "    # Correctly formatted output with extra whitespace around tags.\n",
        "    model_output = (\n",
        "        \"   <think>A 23-year-old man comes to the physician for evaluation of decreased hearing, dizziness, and ringing in his right ear for the past 6 months. Physical examination shows multiple soft, yellow plaques and papules on his arms, chest, and back. There is sensorineural hearing loss and weakness of facial muscles bilaterally. His gait is unsteady. An MRI of the brain shows a 3-cm mass near the right internal auditory meatus and a 2-cm mass at the left cerebellopontine angle. The abnormal cells in these masses are most likely derived from which of the following embryological structures?'</think>   \"\n",
        "        \"   <answer>C</answer>   \"\n",
        "    )\n",
        "    ground_truth = \"C\"\n",
        "    result = verifier.verify(model_output, ground_truth)\n",
        "    print(\"Verification result (should be True):\", result)\n",
        "\n",
        "    # Example with extra non-whitespace text outside of the tags.\n",
        "    model_output_extra = (\n",
        "        \"Intro text that should not be here. \"\n",
        "        \"<think>Detailed reasoning steps.</think><answer>C</answer>\"\n",
        "    )\n",
        "    result_extra = verifier.verify(model_output_extra, ground_truth)\n",
        "    print(\"Verification result with extra text (should be False):\", result_extra)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C86MLIjU_paQ",
        "outputId": "1f646387-412b-4105-e085-73f5f4dc5f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verification result (should be True): True\n",
            "Format error: Extra text found outside of the required tags.\n",
            "Verification result with extra text (should be False): False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GRPO with Unsloth"
      ],
      "metadata": {
        "id": "299BmtMoGJ5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL4bu8lCF8YT",
        "outputId": "c7e7a127-38a0-4efe-e210-be2b5ec6ff9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "max_seq_length = 1024  # Increase if you need longer reasoning traces\n",
        "lora_rank = 32         # Larger rank may improve performance at the cost of speed\n",
        "\n",
        "# Step 1: Load the model and tokenizer using Unsloth's FastLanguageModel.\n",
        "# This loads the model with settings for fast inference (vLLM) and 4-bit quantization.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"meta-llama/meta-Llama-3.1-8B-Instruct\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    load_in_4bit=True,            # Set to False if using LoRA with 16-bit precision instead\n",
        "    fast_inference=True,          # Enable vLLM fast inference for speed\n",
        "    max_lora_rank=lora_rank,      # Ensures that our LoRA modules are configured properly\n",
        "    gpu_memory_utilization=0.6,   # Adjust if you're running out of GPU memory\n",
        ")\n",
        "\n",
        "# Step 2: Configure the model for fine-tuning using PEFT (LoRA).\n",
        "# We attach LoRA modules to key projection layers to enable efficient fine-tuning.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=lora_rank,                  # LoRA rank, suggested values: 8, 16, 32, etc.\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],                            # You may remove some modules (like QKVO) if you experience memory issues\n",
        "    lora_alpha=lora_rank,\n",
        "    use_gradient_checkpointing=\"unsloth\",  # Enables gradient checkpointing for long-context fine-tuning\n",
        "    random_state=3407,                     # For reproducibility\n",
        ")\n",
        "\n",
        "print(\"Model and tokenizer are successfully loaded and configured for GRPO fine-tuning!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 674,
          "referenced_widgets": [
            "8c8bb773b23849a9b0b034c81d4990d9",
            "e98ee76a41504c72b59bf6d28df665c2",
            "9b7811bfc21a4fd5ae957e3808056568",
            "6c1dc470ffc94391b010739e8dfc5154",
            "782be50f3ac546a38eff9d82a43b7c4b",
            "b5107bff48da40e28c6661894074ad39",
            "7446a74db19b47328f15c5e4d46c24af",
            "77d76133e0ff4572949bd344c4c658ca",
            "1d6487dd85d342d89378f0328de360c2",
            "6695c15ee63543c69ed1c0032c0a7536",
            "93f3276f8a5843619af81790488aab3f",
            "64283ab760844eb1858e7cdc540ed8d2",
            "98688bf0279a4c56bc493c2f6a90d8e5",
            "a0c85c443a5e4223b16ca7d88229a825",
            "c6d932823ff04acbbba46d728d5b51ab",
            "326786266ba4440b80a60958cd2b42dd",
            "a1421a600e1c41d2a0ad4054c54932ac",
            "262a0899b68f418aaa1c4f4370fca55f",
            "ba4b15c0cd574f5ebfeb05c884f70089",
            "279b3eb7e59c41c6bd9fb4a9c3e17ef4",
            "58aefd51782c4fba889825b7c9f1e1ba",
            "4638a4e1d4ad4b83966f8c193994a6f5"
          ]
        },
        "id": "ILvqrZvqGfRz",
        "outputId": "3ec2e8b6-8872-4ee7-d08d-344c8b8ec952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-17 15:37:52 __init__.py:207] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.48.3. vLLM: 0.7.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: vLLM loading unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit with actual GPU utilization = 59.43%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 1024. Num Sequences = 160.\n",
            "Unsloth: vLLM's KV Cache can use up to 2.59 GB. Also swap space = 2 GB.\n",
            "WARNING 03-17 15:37:54 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 03-17 15:38:06 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
            "Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_use_double_quant': True, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': ['lm_head', 'multi_modal_projector', 'merger', 'modality_projection', 'model.layers.1.mlp'], 'llm_int8_threshold': 6.0}\n",
            "INFO 03-17 15:38:06 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', speculative_config=None, tokenizer='unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":160}, use_cached_outputs=False, \n",
            "INFO 03-17 15:38:08 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 03-17 15:38:08 cuda.py:226] Using XFormers backend.\n",
            "INFO 03-17 15:38:08 model_runner.py:1110] Starting to load model unsloth/meta-llama-3.1-8b-instruct-unsloth-bnb-4bit...\n",
            "INFO 03-17 15:38:09 loader.py:1089] Loading weights with BitsAndBytes quantization.  May take a while ...\n",
            "INFO 03-17 15:38:09 weight_utils.py:254] Using model weights format ['*.safetensors']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c8bb773b23849a9b0b034c81d4990d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64283ab760844eb1858e7cdc540ed8d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-17 15:38:57 model_runner.py:1115] Loading model weights took 5.5976 GB\n",
            "INFO 03-17 15:38:57 punica_selector.py:18] Using PunicaWrapperGPU.\n",
            "INFO 03-17 15:39:03 worker.py:267] Memory profiling takes 5.51 seconds\n",
            "INFO 03-17 15:39:03 worker.py:267] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.59) = 8.76GiB\n",
            "INFO 03-17 15:39:03 worker.py:267] model weights take 5.60GiB; non_torch_memory takes 0.03GiB; PyTorch activation peak memory takes 0.74GiB; the rest of the memory reserved for KV Cache is 2.39GiB.\n",
            "INFO 03-17 15:39:03 executor_base.py:111] # cuda blocks: 1224, # CPU blocks: 1024\n",
            "INFO 03-17 15:39:03 executor_base.py:116] Maximum concurrency for 1024 tokens per request: 19.12x\n",
            "INFO 03-17 15:39:05 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Capturing CUDA graph shapes: 100%|██████████| 23/23 [00:39<00:00,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 03-17 15:39:44 model_runner.py:1562] Graph capturing finished in 39 secs, took 0.59 GiB\n",
            "INFO 03-17 15:39:44 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 47.64 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Unsloth 2025.3.14 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer are successfully loaded and configured for GRPO fine-tuning!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reformats a verifiable dataset"
      ],
      "metadata": {
        "id": "ErlnUsw5ZSs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the system prompt that instructs the model to use a specific format.\n",
        "SYSTEM_PROMPT = \"\"\"\n",
        "Respond in the following format:\n",
        "<reasoning>\n",
        "...\n",
        "</reasoning>\n",
        "<answer>\n",
        "...\n",
        "</answer>\n",
        "\"\"\"\n",
        "\n",
        "# Define the XML Chain-of-Thought (CoT) format template.\n",
        "XML_COT_FORMAT = \"\"\"\\\n",
        "<reasoning>\n",
        "{reasoning}\n",
        "</reasoning>\n",
        "<answer>\n",
        "{answer}\n",
        "</answer>\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "u8KWqOzeMZ89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_verifiable_item(item):\n",
        "    \"\"\"\n",
        "    Reformats a verifiable dataset item into a training example with a prompt and target output.\n",
        "\n",
        "    Each item is expected to have:\n",
        "      - 'x': The problem statement.\n",
        "      - 'y_star': The ground-truth answer.\n",
        "\n",
        "    The prompt includes a system instruction (SYSTEM_PROMPT) followed by the problem text.\n",
        "    The target output follows the XML_COT_FORMAT, with a placeholder for reasoning and the ground-truth answer.\n",
        "    \"\"\"\n",
        "    # Create the prompt by concatenating the system instruction and the problem statement.\n",
        "    prompt = SYSTEM_PROMPT.strip() + \"\\n\\n\" + item['x']\n",
        "\n",
        "    # The target output encourages the model to explain its reasoning.\n",
        "    # Since our dataset might not have explicit reasoning annotations, we insert a placeholder.\n",
        "    target = XML_COT_FORMAT.format(reasoning=\"(explain your reasoning here)\", answer=item['y_star'])\n",
        "\n",
        "    return {\"prompt\": prompt, \"target\": target}\n",
        "\n",
        "# Example: Reformatting a verifiable dataset (list of dicts with 'x' and 'y_star' keys).\n",
        "# Assume verifiable_dataset is already defined and populated.\n",
        "formatted_dataset = [format_verifiable_item(item) for item in verifiable_dataset]\n",
        "\n",
        "# Print one example to inspect the formatting.\n",
        "example = formatted_dataset[0]\n",
        "print(\"Prompt:\\n\", example[\"prompt\"])\n",
        "print(\"Target Output:\\n\", example[\"target\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoIyrTp2ND9V",
        "outputId": "298b0752-7825-48b6-872d-ed7413a3dd18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt:\n",
            " Respond in the following format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer>\n",
            "\n",
            "A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
            "Target Output:\n",
            " <reasoning>\n",
            "(explain your reasoning here)\n",
            "</reasoning>\n",
            "<answer>\n",
            "Nitrofurantoin\n",
            "</answer>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reward Functions"
      ],
      "metadata": {
        "id": "y1FMH3XLaRBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Union\n",
        "\n",
        "# Helper: Extract answer from the <answer> tag.\n",
        "def extract_answer_from_output(model_output: str) -> Union[str, None]:\n",
        "    match = re.search(r\"<answer>(.*?)</answer>\", model_output, re.DOTALL)\n",
        "    return match.group(1).strip() if match else None\n",
        "\n",
        "# 1. Reward function that checks if the answer is correct.\n",
        "def reward_correct_answer(prompts: List[str], completions: List[str], ground_truths: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    For each example, returns 1.0 if the extracted answer exactly matches the ground truth; 0.0 otherwise.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for comp, gt in zip(completions, ground_truths):\n",
        "        answer = extract_answer_from_output(comp)\n",
        "        rewards.append(1.0 if answer == gt else 0.0)\n",
        "    return rewards\n",
        "\n",
        "# 2. Reward function that checks if the answer is an integer.\n",
        "def reward_integer_answer(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    For each example, returns 1.0 if the extracted answer can be parsed as an integer; 0.0 otherwise.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for comp in completions:\n",
        "        answer = extract_answer_from_output(comp)\n",
        "        if answer is None:\n",
        "            rewards.append(0.0)\n",
        "        else:\n",
        "            try:\n",
        "                int(answer)\n",
        "                rewards.append(1.0)\n",
        "            except ValueError:\n",
        "                rewards.append(0.0)\n",
        "    return rewards\n",
        "\n",
        "# 3. Reward function that checks if the completion follows the strict format.\n",
        "def reward_strict_format(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    For each example, returns 1.0 if the output contains exactly one <think>...</think> block,\n",
        "    exactly one <answer>...</answer> block, and no extra text; 0.0 otherwise.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for comp in completions:\n",
        "        think_matches = re.findall(r\"<think>(.*?)</think>\", comp, re.DOTALL)\n",
        "        answer_matches = re.findall(r\"<answer>(.*?)</answer>\", comp, re.DOTALL)\n",
        "        cleaned_output = re.sub(r\"<think>.*?</think>\", \"\", comp, flags=re.DOTALL)\n",
        "        cleaned_output = re.sub(r\"<answer>.*?</answer>\", \"\", cleaned_output, flags=re.DOTALL).strip()\n",
        "        rewards.append(1.0 if (len(think_matches) == 1 and len(answer_matches) == 1 and cleaned_output == \"\") else 0.0)\n",
        "    return rewards\n",
        "\n",
        "# 4. Reward function that checks if the completion follows a more relaxed format.\n",
        "def reward_relaxed_format(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    For each example, returns 1.0 if the output contains exactly one <think> block and one <answer> block,\n",
        "    ignoring extra whitespace or minor text differences; 0.0 otherwise.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for comp in completions:\n",
        "        think_matches = re.findall(r\"<think>(.*?)</think>\", comp, re.DOTALL)\n",
        "        answer_matches = re.findall(r\"<answer>(.*?)</answer>\", comp, re.DOTALL)\n",
        "        rewards.append(1.0 if (len(think_matches) == 1 and len(answer_matches) == 1) else 0.0)\n",
        "    return rewards\n",
        "\n",
        "# 5. Reward function that counts XML tags and penalizes extra content.\n",
        "def reward_xml_tag_penalty(prompts: List[str], completions: List[str], **kwargs) -> List[float]:\n",
        "    \"\"\"\n",
        "    For each example, starts with a base reward of 1.0 if the required tags (<think> and <answer>)\n",
        "    are present exactly once. Then penalizes extra XML tags and any extra non-tag content.\n",
        "    \"\"\"\n",
        "    rewards = []\n",
        "    for comp in completions:\n",
        "        required_tags = [\"<think>\", \"</think>\", \"<answer>\", \"</answer>\"]\n",
        "        counts = {tag: comp.count(tag) for tag in required_tags}\n",
        "        base_reward = 1.0 if all(counts[tag] == 1 for tag in required_tags) else 0.0\n",
        "        if base_reward == 0.0:\n",
        "            rewards.append(0.0)\n",
        "            continue\n",
        "\n",
        "        all_tags = re.findall(r\"<.*?>\", comp)\n",
        "        extra_tags = len(all_tags) - 4  # Expect exactly 4 required tags.\n",
        "        penalty_tags = 0.1 * max(extra_tags, 0)\n",
        "\n",
        "        cleaned_output = re.sub(r\"<think>.*?</think>\", \"\", comp, flags=re.DOTALL)\n",
        "        cleaned_output = re.sub(r\"<answer>.*?</answer>\", \"\", cleaned_output, flags=re.DOTALL).strip()\n",
        "        penalty_text = 0.1 if cleaned_output != \"\" else 0.0\n",
        "\n",
        "        total_penalty = penalty_tags + penalty_text\n",
        "        final_reward = max(base_reward - total_penalty, 0.0)\n",
        "        rewards.append(final_reward)\n",
        "    return rewards"
      ],
      "metadata": {
        "id": "Ci5HmT5IYY_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Training with GRPO"
      ],
      "metadata": {
        "id": "E5iW14VvPIQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRPO Trainer Setup\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "# Define maximum prompt length and derive completion length.\n",
        "max_prompt_length = 256\n",
        "max_seq_length = 1024  # from our model setup earlier\n",
        "max_completion_length = max_seq_length - max_prompt_length\n",
        "\n",
        "# Set up the GRPO configuration.\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate=5e-6,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.99,\n",
        "    weight_decay=0.1,\n",
        "    warmup_ratio=0.1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    optim=\"paged_adamw_8bit\",  # Use 8-bit optimizer to reduce memory usage.\n",
        "    logging_steps=1,\n",
        "    per_device_train_batch_size=6,  # Must be a multiple of num_generations.\n",
        "    gradient_accumulation_steps=1,\n",
        "    num_generations=6,             # Number of candidates generated per prompt.\n",
        "    max_prompt_length=max_prompt_length,\n",
        "    max_completion_length=max_completion_length,\n",
        "    max_steps=10,   # For testing purposes; adjust as needed.\n",
        "    save_steps=10,  # For testing purposes; adjust as needed.\n",
        "    max_grad_norm=0.1,\n",
        "    report_to=\"wandb\",             # Change to \"none\" if not using Weights & Biases.\n",
        "    output_dir=\"outputs\",\n",
        ")\n",
        "\n",
        "# Define the list of reward functions.\n",
        "reward_funcs = [\n",
        "    reward_xml_tag_penalty,\n",
        "    reward_relaxed_format,\n",
        "    reward_strict_format,\n",
        "    reward_integer_answer,\n",
        "    # Wrap the correctness reward to pass ground truths.\n",
        "    lambda prompts, completions, target, **kwargs: reward_correct_answer(prompts, completions, ground_truths=target, **kwargs),\n",
        "]"
      ],
      "metadata": {
        "id": "-zBZ1e9aY_tZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRPO Trainer Initialization\n",
        "trainer = GRPOTrainer(\n",
        "    model=model,                    # Our fine-tuned model.\n",
        "    processing_class=tokenizer,     # The associated tokenizer.\n",
        "    reward_funcs=reward_funcs,\n",
        "    args=training_args,\n",
        "    train_dataset=formatted_dataset,  # Our dataset in role-based prompt format.\n",
        ")\n",
        "\n",
        "# Start Training\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "1IJn5TdWXuW7",
        "outputId": "828aa74f-6698-43fa-e92f-c00d388a4170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 33,929 | Num Epochs = 1 | Total steps = 10\n",
            "O^O/ \\_/ \\    Batch size per device = 6 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (6 x 1 x 1) = 6\n",
            " \"-____-\"     Trainable parameters = 83,886,080/8,000,000,000 (1.05% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250317_154225-37p315y8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ashaduzzaman_sarker/huggingface/runs/37p315y8' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/ashaduzzaman_sarker/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ashaduzzaman_sarker/huggingface' target=\"_blank\">https://wandb.ai/ashaduzzaman_sarker/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ashaduzzaman_sarker/huggingface/runs/37p315y8' target=\"_blank\">https://wandb.ai/ashaduzzaman_sarker/huggingface/runs/37p315y8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [10/10 27:33, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>reward</th>\n",
              "      <th>reward_std</th>\n",
              "      <th>completion_length</th>\n",
              "      <th>kl</th>\n",
              "      <th>rewards / reward_xml_tag_penalty</th>\n",
              "      <th>rewards / reward_relaxed_format</th>\n",
              "      <th>rewards / reward_strict_format</th>\n",
              "      <th>rewards / reward_integer_answer</th>\n",
              "      <th>rewards / <lambda></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>729.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>765.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>678.833374</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>686.666687</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>709.833374</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>523.833374</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>516.833374</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>625.833374</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=10, training_loss=2.19791091851107e-06, metrics={'train_runtime': 1844.5457, 'train_samples_per_second': 0.033, 'train_steps_per_second': 0.005, 'total_flos': 0.0, 'train_loss': 2.19791091851107e-06})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_lora(\"grpo_saved_lora\")"
      ],
      "metadata": {
        "id": "HjWdt607Ubu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing the Model"
      ],
      "metadata": {
        "id": "DlRbcUWPUbK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_dataset[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1Z1iduc4t0V",
        "outputId": "b830d3fc-8574-420d-c18d-c710f0283334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'prompt': 'Respond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n\\nA 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?',\n",
              "  'target': '<reasoning>\\n(explain your reasoning here)\\n</reasoning>\\n<answer>\\nNitrofurantoin\\n</answer>\\n'},\n",
              " {'prompt': 'Respond in the following format:\\n<reasoning>\\n...\\n</reasoning>\\n<answer>\\n...\\n</answer>\\n\\nA 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?',\n",
              "  'target': '<reasoning>\\n(explain your reasoning here)\\n</reasoning>\\n<answer>\\nPlacing the infant in a supine position on a firm mattress while sleeping\\n</answer>\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from vllm import SamplingParams\n",
        "\n",
        "# Select a random example from your verifiable dataset.\n",
        "sample = random.choice(formatted_dataset)\n",
        "\n",
        "# Prepare the test input.\n",
        "if isinstance(sample[\"prompt\"], list):\n",
        "    test_text = tokenizer.apply_chat_template(\n",
        "        sample[\"prompt\"],\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "    )\n",
        "else:\n",
        "    test_text = sample[\"prompt\"]\n",
        "\n",
        "# Set up sampling parameters for generation.\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    max_tokens=1024,\n",
        ")\n",
        "\n",
        "# Generate the model output using the saved LoRA weights.\n",
        "output = (\n",
        "    model.fast_generate(\n",
        "        test_text,\n",
        "        sampling_params=sampling_params,\n",
        "        lora_request=model.load_lora(\"grpo_saved_lora\"),\n",
        "    )[0]\n",
        "    .outputs[0]\n",
        "    .text\n",
        ")\n",
        "\n",
        "# Print the test prompt, the ground-truth target, and the model output.\n",
        "print(\"Test Prompt:\")\n",
        "print(test_text)\n",
        "print(\"\\nGround Truth Target:\")\n",
        "print(sample[\"target\"])\n",
        "print(\"\\nModel Output:\")\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "7idclrXdVuHp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1db989f5-0443-4482-f06d-52956db5bde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processed prompts: 100%|██████████| 1/1 [00:52<00:00, 52.28s/it, est. speed input: 1.51 toks/s, output: 18.09 toks/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Prompt:\n",
            "Respond in the following format:\n",
            "<reasoning>\n",
            "...\n",
            "</reasoning>\n",
            "<answer>\n",
            "...\n",
            "</answer>\n",
            "\n",
            "A person 'X' hits another person 'Y' with a wooden stick on provocation. This leads to the formation of a bruise 3 cm × 3 cm on the forearm. No other injuries are noted. Which of the following is true, regarding his punishment -\n",
            "\n",
            "Ground Truth Target:\n",
            "<reasoning>\n",
            "(explain your reasoning here)\n",
            "</reasoning>\n",
            "<answer>\n",
            "A\n",
            "</answer>\n",
            "\n",
            "\n",
            "Model Output:\n",
            " \n",
            "\n",
            "A) He should be given bail.\n",
            "B) Cognizable and non-bailable offence\n",
            "C) Cognizable and bailable offence\n",
            "D) The offence should be tried as Magistrate's Court.\n",
            "\n",
            "<reasoning>\n",
            "The criminal law relating to the punishment of the person 'X' depends on whether the offence falls under the categories of 'Cognizable or non-cognizable', 'Cognizable and bailable or non-bailable' and the relevant court to try the case. The given situation is hitting another person with a wooden stick on provocation. In criminal law, such a situation can be classified under Section 325 or 324 of IPC, which deals with 'Voluntarily causing grievous hurt' and 'Voluntarily causing hurt'. Since a bruise of 3 cm × 3 cm was formed, it is considered as a 'hurt'. Hence, it can be classified as a non-cognizable offence under Section 323 of IPC. Moreover, it is also a bailable offence. This would mean that the relevant court to try such cases are the courts of the Judicial Magistrate First Class.\n",
            "</reasoning>\n",
            "<answer>\n",
            "C) Cognizable and bailable offence.\n",
            "</answer>  This question requires the student to apply the criminal law to the given situation and classify the offence under the appropriate section of the Indian Penal Code (IPC). The student must analyze the situation, identify the relevant section of the IPC, and classify the offence as 'cognizable or non-cognizable', 'bailable or non-bailable' and the relevant court to try the case. The correct answer is C) Cognizable and bailable offence. \n",
            "The correct answer is C) Cognizable and bailable offence. \n",
            "The correct answer is C) Cognizable and bailable offence.  This question requires the student to apply the criminal law to the given situation and classify the offence under the appropriate section of the Indian Penal Code (IPC). The student must analyze the situation, identify the relevant section of the IPC, and classify the offence as 'cognizable or non-cognizable', 'bailable or non-bailable' and the relevant court to try the case. The correct answer is C) Cognizable and bailable offence. \n",
            "\n",
            "The correct answer is C) Cognizable and bailable offence.  The correct answer is C) Cognizable and bailable offence.  This question requires the student to apply the criminal law to the given situation and classify the offence under the appropriate section of the Indian Penal Code (IPC). The student must analyze the situation, identify the relevant section of the IPC, and classify the offence as 'cognizable or non-cognizable', 'bailable or non-bailable' and the relevant court to try the case. The correct answer is C) Cognizable and bailable offence. \n",
            "\n",
            "The correct answer is C) Cognizable and bailable offence.  This question requires the student to apply the criminal law to the given situation and classify the offence under the appropriate section of the Indian Penal Code (IPC). The student must analyze the situation, identify the relevant section of the IPC, and classify the offence as 'cognizable or non-cognizable', 'bailable or non-bailable' and the relevant court to try the case. The correct answer is C) Cognizable and bailable offence. \n",
            "The correct answer is C) Cognizable and bailable offence.  The correct answer is C) Cognizable and bailable offence. \n",
            "\n",
            "The correct answer is C) Cognizable and bailable offence.  This question requires the student to apply the criminal law to the given situation and classify the offence under the appropriate section of the Indian Penal Code (IPC). The student must analyze the situation, identify the relevant section of the IPC, and classify the offence as 'cognizable or non-cognizable', 'bailable or non-bailable' and the relevant court to try the case. The correct answer is C) Cognizable and bailable offence. \n",
            "The correct answer is C) Cognizable and bailable offence.  The correct answer is C) Cognizable and bailable offence. \n",
            "\n",
            "The correct answer is C) Cognizable and bailable offence.  This question requires the student to apply the criminal law to the given situation and classify the offence under the appropriate section of the Indian Penal Code (IPC). The student must analyze the situation, identify the relevant section of the IPC, and classify the offence as 'cognizable or non-cognizable', 'bailable or non-bailable'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save the model"
      ],
      "metadata": {
        "id": "yCIuI9hyzNiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save to 16-bit precision\n",
        "# model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")"
      ],
      "metadata": {
        "id": "X1XOH6sjzQPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Benchmark Evaluation"
      ],
      "metadata": {
        "id": "9e03-6G58PVQ"
      }
    }
  ]
}